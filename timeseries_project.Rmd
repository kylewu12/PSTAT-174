---
title: "The Effects of Economic Factors on Total Vehicles Sold in the USA"
author: "Kyle Wu"
date: "2023-01-30"
bibliography: bib.bib
output:
  pdf_document: default
  fontsize: 14pt
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE, message = FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
```

```{r, message = FALSE, include=FALSE}
library(fpp3)
library(feasts)
library(tsibble)
library(fable)
library(seasonal)
library(tidyverse)
library(tsibble)
library(MASS)
library(gridExtra)
library(forecast)
library(astsa)
library(urca)
library(gt)
library(gtExtras)
library(kableExtra)
library(AER)
library(quantmod)
library(dynlm)
library(stargazer)
library(scales)
library(ARDL)
library(vars)
```

```{r, show_col_types = FALSE, include = FALSE}
#All data read in
data <- readr::read_csv("fredgraph.csv")

data <- data %>%
  mutate(Month = yearmonth(DATE)) %>%
  as_tsibble(index = Month) %>%
  filter(DATE > "1992-12-01") %>%
  filter(DATE < "2023-01-01")
```

```{r, message = FALSE}
#Data read in individually 
auto_production <- readr::read_csv("DAUPNSA.csv")
total_vehicles <- readr::read_csv("TOTALNSA.csv")
fuel_prices <- readr::read_csv("CUSR0000SETB01.csv")
bank_rate <- readr::read_csv("MPRIME.csv")
unemployment <- readr::read_csv("UNRATE.csv")
```

```{r}
unemployment <- unemployment %>%
  dplyr::mutate(Month = yearmonth(DATE)) %>%
  as_tsibble(index = Month) 

unemp_ts <- autoplot(unemployment, UNRATE) +
  labs(title = "Unemployment Rate", y = "Unemployment Rate")
```

```{r}
bank_rate <- bank_rate %>%
  mutate(Month = yearmonth(DATE)) %>%
  as_tsibble(index = Month) 

bank_rate_ts <- autoplot(bank_rate, MPRIME) +
  labs(title = "Bank Prime Loan Rate", y = "%")
```

```{r}
total_vehicles <- total_vehicles %>%
  mutate(Month = yearmonth(DATE)) %>%
  as_tsibble(index = Month) 

total_vehicles_ts <- autoplot(total_vehicles, TOTALNSA) +
  labs(title = "Total Vehicle Sales", y = "Thousands of Units")
```

```{r}
auto_production <- auto_production %>%
  mutate(Month = yearmonth(DATE)) %>%
  as_tsibble(index = Month) 

auto_production2 <- auto_production %>%
  mutate(Month = yearmonth(DATE)) %>%
  as_tsibble(index = Month) %>% filter(DATE <= "2020-01-01")


auto_production_ts <- autoplot(auto_production, DAUPNSA) +
  labs(title = "Domestic Auto Production", y = "Thousands of Units")
```

```{r}
fuel_prices <- fuel_prices %>%
  mutate(Month = yearmonth(DATE)) %>%
  as_tsibble(index = Month) %>%
  filter(DATE > "1968-01-01")
  

fuel_prices_ts <- autoplot(fuel_prices, CUSR0000SETB01) +
  labs(title = "Gas CPI", y = "Index 1982-1984 = 100")
```

```{r}
combined <- reduce(list(bank_rate, auto_production, total_vehicles, unemployment, fuel_prices), left_join, by = "DATE") 

combined <- as_tsibble(combined) %>%
  dplyr::select(DATE, MPRIME, DAUPNSA, TOTALNSA, UNRATE, CUSR0000SETB01, Month)
```

## Data Application

In the United States, one of the main modes of transportation is the
automobile. To the average consumer, it has seemed that the new car has
been slowly getting out of reach, with the average price of a new
vehicle currently sitting around \$49,500 [@carprice2023]. Despite the
high prices of vehicles for many Americans a car is not only a luxury,
but a necessity, and many citizens find themselves shelling out a large
portion of their paychecks for their transportation. Even when
individuals opt to purchase used cars, they are still often faced with
prices that would have seemed exorbitant not that long ago.

Since this is the case, studying the United States car market over time
will allow us to gain useful knowledge that will be of significance not
only to the average consumer, but also for economists trying to
understand what trends the American auto market may be facing going
forwards and what factors influence automotive sales. Past research into
the American auto market have been vital to our understanding of the
forces driving the auto market. For example, it is well known that the
chip shortage that occurred as a result of COVID-19 shutdowns, among
other reasons led to a chip shortage that has in many ways created
problems for the world economy [@jpm2022]. Since cars now heavily rely
on computers to work, this resulted in many manufacturers around the
world decreasing production projections, which decreased vehicle
production, and partially led to the rapid rise in vehicle prices.
However, if we look at production figures, we can see that the domestic
production of cars had been following a decreasing trend since the 90s,
so researchers at Federal Reserve Economic Data (FRED) found that it is
hard to say if COVID was fully responsible for the decreased production,
or if it would have happened regardless [@FRED2022]. Research by FRED
also indicated that despite the increase in population since the mid
1970s, the total number of vehicles sold has remained relatively flat
aver the past few decades [@FRED2021].

Looking at the data offered by the Federal Reserve could allow us to
answer even more questions regarding the American auto market. For
example, we could try to understand if it is likely that american
automakers would have decreased their production numbers even without
the disruptions brought about by COVID or if COVID led to new trends. If
we take into account other economic factors, such as interest rate or
gas prices, we can then try to measure what economic factors may most
affect the sale of motor vehicles. Using the data we obtained and after
determing factors that determine automotive sales, we can then create a
forecast to determine how each factor relating to the automotive
industry will change in the future. For example, we can try to answer
the question of whether it is likely new vehicles will continue facing
inflation or if it might become stable in the near future. Besides
looking at the various factors individually, we can also look at the
auto market holistically, asking what the future may be in terms of
vehicle purchases in the United States and is it likely that vehicle
purchases return to pre-COVID levels. For the average consumer, the
questions that will be answered will allow them to perhaps better plan
for the expenditure that comes with the purchase of a new car.

Furthermore, studying time series data of vehicles can allow us to
better understand how or if certain policy changes may change vehicle
prices or purchasing behavior. For example, we could potentially find
time periods with varying federal funds rates, which influence bank
prime loan rates to see if this changed the overall behavior of
consumers.

Gathering all this data about the American auto market would then allow
us to broadly gain an understanding not only of factors affecting
vehicle sales, but also of the health of the American economy due to the
fact that vehicles are often the second most expensive possessions of
individuals, second only to homes. Increased purchasing of vehicles
would indicate that the American has been healthy and following a
positive trend, whereas decreased vehicle purchases may indicate that
the economy had been following a general downwards trajectory.

## Analysis of Empirical Properties

In all cases, the data I selected came from the Federal Reserve Economic
Data database and all variables selected were recorded on a monthly
basis and input into a format that was very neat and effective for time
series analysis. The variables I have chosen are Total number of
vehicles sold, new vehicle consumer price index, domestic auto
production, fuel price index, and the bank prime rate. In this study, I
will use total number of vehicles sold as the gauge of the american auto
market, and the other variables will be used as predictors.

Since we will be creating time series regression models, we first want
to get an overview of the data to see how each variable relates to the
other, which we can do through a correlation plot.

```{r, message=FALSE, warning=FALSE, fig.cap= "Correlation Plot."}
GGally::ggpairs(combined[,2:6])
```

From the correlation plot, we can see that all the variables have some
form of statistical significance with the exception of the correlation
between the gas price index and the bank prime rate loans. This would
make sense because there really shouldn't be a reason why there should
be a significant correlation between the variables especially
considering that bank prime rate loans are assocaited with larger
purchases such as for cars or houses.

We will now analyze the variables individually before talking about all
the factors as they may relate to projecting future car sales.

The first variable we will look at is total vehicle sales.

```{r, fig.cap= "Total New Vehicle Sales time series plot (Top) and Total Vehicle Sales seasonal subseries plot (Bottom)."}
tvsubseries <- total_vehicles %>%
  gg_subseries(TOTALNSA) + 
  labs(y = "Thousands of Vehicles Sold", title = "Total Vehicle Sales")

total_sub <- total_vehicles %>%
  gg_subseries(TOTALNSA) + 
  labs( y = "Thousands of Units", title = "Total vehicles sold")

tvacf <- total_vehicles %>%
  ACF(TOTALNSA) %>%
  autoplot() + labs(title = "Total Vehicles Sold")

grid.arrange(total_vehicles_ts, total_sub)
```

Our data for vehicles sold starts from January 1976, is recorded
monthly, and does not have any missing values.The data was collected
from the U.S. Bureau of Economic Analysis, which has formatted the data
in a easily usable database. From the subseries plot, we see that the
data definitely follows a seasonal pattern, as demonstrated by the
fluctuating mean lines based on the month of the year. Additionally,
from the scalloped shape of the plot, we can see that the data follows a
seasonal trend of peaks and troughs every twelve months. Logically this
makes sense, as it is well known that vehicle sales usually increase
during the summer and tend to decrease during the winter. From the data
we can determine the number of vehicles sold over the last 5 decades and
whether there has been a general trend in vehicle sales.

We can now look at the consumer price index of new vehicles in the
United States.

```{r, fig.cap="Time series plot of Gasoline CPI (Top) and Unemployment Rate (Bottom)."}
grid.arrange(fuel_prices_ts, unemp_ts)
```

Records for the price index of gasoline start from February 1968 and are
recorded monthly by the U.S. Bureau of Labor Statistics with no
missingness present. From the data, we see that there has been a general
increase in the price of gasoline from 1968 until the early 2000s before
prices seem to level off, but with high volatility. We also see that
following COVID, there was a large spike in gasoline prices, which has
since gradually come down. This data will allow us to understand what
trends are present in terms of gasoline prices and whether gasoline
prices have an effect on the total number of vehicles sold. In other
words, do high gasoline prices lead to less consumers buying vehicles?

The data for unemployment rate goes back to January 1948 and is recorded
on a monthly basis and is collected by the U.S. Bureau of Labor
Statistics and there is no missingess present in this time series. From
our data, we see general cyclical fluctiations with periods with varying
periods of higher vs lower unemployment. There does not appear to have
been a consistent trend of unemployment over the lifecycle of the data.
In more recent times, we see that there has been a decrease in the
unemployment rate since after the Great Recession following 2008 with a
large spike in unemplopyment following the beginning stages of the
COVID-19 pandemic. We can use the unemployment data to determine whether
there is a relationship between rates of unemployment and the number of
total vehicles sold in the United States, and if a relationship is
present, we can attempt to determine the strength of the relationship.

```{r, fig.cap="Time series plot of Domestic Auto Production (Top) and Domestic Auto Production Seasonal Plot (Bottom)."}
production_season <- auto_production %>% 
  gg_season(DAUPNSA, polar = TRUE)

grid.arrange(auto_production_ts, production_season)
```

For Domestic Auto Production, we have monthly recorded data from the
U.S. Bureau of Economic Analysis with no missingess present in the data.
Overall, we appear to see volatility month-to-month and pretty
consistent seasonal patterns with July being a month with consistently
low production numbers in relation to other months and March and October
having relatively high production numbers. This data alone can answer a
couple questions about the United States auto market. First of all, we
can answer if the COVID induced supply shortages drastically impacted
U.S. vehicle production numbers or if, as suggested by FRED COVID simply
highlighted a long-term trend that would have occurred regardless. Using
this data will then also allow us to make forecasts on what the future
outlook may be for domestic auto production.

```{r, fig.cap="Bank Prime Rate Loan."}
bank_rate_ts
```

The Bank Prime loan rate is set in relation to the federal funds rate
set by the federal reserve and we have data going back to January 1949.
The rate is often set based on other determinants of the market so we
may not be able to generate too many insights from it individually.
However, if we use the bank prime rate in addition to the other factors
we discussed earlier, we may be able to create a regression model that
will allow us to understand what economic factors influence the car
purchasing decisions of Americans. Additionally, studying all the data
together will allow us to understand if COVID-19 disruptions altered the
purchasing decisions of Americans or if there were vehicle market trends
that would have likely occurred regardless of the pandemic.

\newpage

## Models for Data Fitting

Throughout this case study, we will aim to fit time series regression
models that allow us to understand the relationships between the total
number of vehicles sold and different economic variables. The selected
will be Autoregressive Distributed Lag (ARDL) models, which use a
combination of self lagged values, distributed lags, seasonality
components, and trend components. Self lagged values means the model
takes into account previous values of itself to make future predictions.
Distributed lags means that the regression model takes into account
previous values of an independent variable to make predictions. Finally,
seasonality and trend components are extracted from the data itself. In
order to construct an ARDL model, we must fiest examine the data and
determine if the data is stationary either at I(0) or I(1). If the data
is stationary, we can then begin to fit our models using criteria such
as the Akaike Information Criterion (AIC) to select the best model.
Finally, we will have to use diagnostic checking where we will examine
the residuals from the fitted model to see if they appear sufficient. If
we initially appear to fit models that do not fit the data properly, we
should then go back and once again attempt to find an appropriate model.

The first step to fitting our model is to see if any of our time series
need to be transformed or adjusted, which often allows us to analyze
simpler time series. Adjusting the data will allow us to make patterns
more consistent across our data, which will allow us to better model the
data, and will lead to better forecasts. For the metrics I used, the
only ones that may need transformations due to variations in seasonality
are those for domestic auto production, total vehicles sold, and the
gasoline price index. Since these time series show variations that
change with the level of the series, in order to allow the data to
remain interpretable, we will apply logarithmic transformations.

```{r, message=FALSE, fig.cap="Log adjusted plot for Total New Vehicle Sales (Top), Domestic Auto Production (Middle), and Gasoline CPI (Bottom)."}
total_adjusted <- total_vehicles %>%
  autoplot(log(TOTALNSA))

production_adjusted <- auto_production %>%
  autoplot(log(DAUPNSA))

fuel_adjusted <- fuel_prices %>% 
  autoplot(log(CUSR0000SETB01))

grid.arrange(total_adjusted, production_adjusted, fuel_adjusted)
```

The next step to fitting our model is to decompose our data into the
time series' separate components so that we can perform further
analysis. Depending on the type of data we have, there are two possible
ways that we can decompose the data. The first possible case of data is
a time series with a trend but no seasonal variations, which follows the
form $X_t = \alpha + \beta t + \epsilon_t$, where $\alpha, \beta$ are
constants and $\epsilon_t$ is a random error term with mean zero. Since
our CPI data for gasoline prices and new car prices are both seasonally
adjusted, this is the type of decomposition they will undergo.

The second possible case of data is a time series that contains both a
trend, and seasonal variation. In this case, there are two possible
cases of decomposition we can take. If we assume additive decomposition,
then we can use $y_t = S_t + T_t + R_t$, where $S_t$ is the seasonal
component, $T_t$ is the trend component, and $R_t$ is the remainder
component. The other possible case involves multiplicative decomposition
which follows the form $Y_t = S_t \times T_t \times R_t$. We use the
multiplicative case if the variation around the trend-cycle appears to
be proportional to the level of the time series [@partiv]. The additive
case would be if the magnitude of the seasonal fluctuations don't
significantly vary over time. Since we used a box-cox transformation to
stabilize our variance, we can use the additive case in our
decomposition.

In our case, we will use an X-11 decomposition, which is commonly used
by the U.S. Census Bureau. X-11 decomposition works by estimating the
trend by a moving average, removing the trend to leave the seasonal and
irregular components, and estimating the seasonal component using the
moving averages to smooth out irregularities. Due to the fact that
seasonality cannot be identified without the trend and a trend cannot be
estimated without data beign seasonally adjusted, X-11 models use an
iterative approach to come up with the best solution. An example of X-11
decomposition is shown below

```{r, fig.cap="X-11 decomposition of Total New Vehicle Sales"}
fit <- ts(log(total_vehicles$TOTALNSA), start = c(1976,01), frequency = 12) %>% seas(x11 = "")

autoplot(fit)
```

From the decomposition of this time series, along with the other time
series, we can get a general idea of the trend and seasonality present
within the plot, which will then allow us to better estimate and thus
forecast the data moving forwards. Additionally, following the
decomposition, we can then see how the trend and seasonally adjusted
versions of our data fit with the original data.

```{r, fig.cap="Total New Vehicles Sold with trend and seasonally adjusted graph overlayed."}
total_auto <- ts(log(total_vehicles$TOTALNSA), start = c(1976,01), frequency = 12)

autoplot(total_auto, series = "Data") +
  autolayer(trendcycle(fit), series = "Trend") + 
  autolayer(seasadj(fit), series = "Seasonally Adjusted") + 
  xlab("Year") + ylab("Vehicles Sold (Thousands of Units)") + 
  ggtitle("Total New Vehciles Sold") +
  scale_color_manual(values = c("gray", "blue", "red"),
                                      breaks = c("Data", "Seasonally Adjusted", "Trend"))
```

For this case study, our goal is to eventually forecast future values of
each of our time series data through the use of ARDL models. The goal of
ARDL models is to describe autocorrelations in the data for forecasting
[@forecastbook]. When attempting to fit ARDL models, we attempt to first
remove any trend or seasonality present within the data so that we can
create stationary time series, which will allow us to attempt to model
the remaining residuals. Stationary time series are time series whose
mean and variance is constant over time [@kpssstat].

One way to convert non-stationary time series into stationary time
series is the method of differencing, where we compute the differenece
between consecutive observations in a time series and can be written as
follows $$y_t' = y_t - y_{t-1}$$ Differencing time series allows us to
stabilize our means because it removes or reduces trend and seasonality.
It is important to note that for ARDL models, it is important to ensure
that all data is stationary at either the I(0) or I(1) level, meaning
that either the original data must be stationary or the first difference
of the data is stationary. Additionally, for the dependent variable, we
cannot interpret long run relationships of the model unless it is I(1).

Before we start differencing our data, we should first test if our data
is already stationary, which would then allow us to go straight into
further analysis. The test we will use for stationarity is the
Kwiatkowski-Phillip-Schmidt_shin (KPSS) test. The KPSS test works by
breaking up a time series down into the following decomposition
$$Y_t = r_t + \beta_t + \epsilon_t$$ where $r_t$ is a random walk,
$\beta_t$ is the trend, and $\epsilon_t$ is the stationary error. The
KPSS test also involves hypothesis testing with
$$H_0:Y_t\text{ is trend (or level) stationary}$$
$$H_1: Y_t \text{ is a unit root process}$$ where $H_0$ and $H_1$ is the
null and alternative hypothesis respectively. Setting our significance
level at e can now test each time series to determine whether they will
require differencing.

To double check the stationarity of our data, we will also use the
Augmented Dickey-Fuller (ADF) test. ADF tests consider the following
model
$$Y_t = \mu + \beta t + \alpha Y_{t-1} + \sum_{j=1}^p\phi_j\Delta Y_{t-j} + \epsilon_t$$
and they are unit root tests that test the null hypothesis that
$\alpha = 1$ with $p = 0$ where $\alpha$ is the coefficient of the first
lag on $Y$ and $Y_{t-j}$ is the first lag, with $\Delta Y_{t-j}$ first
difference in series at time $t-j$ ADF tests have a null and alternative
hypothesis as follows
$$H_0: \text{Series is non-stationary or series has a unit root}$$
$$H_A: \text{Series is stationary or series has no unit root.}$$ If out
test statistic results in a p-value \< 0.05, we will reject the null
hypothesis that our time series does not have a unit root, which would
mean that it would be stationary and does not hage a time-dependent
structure.

The value of the t-statistic for each KPSS test is as follows:

```{r, include=FALSE}

# KPSS Tests for Time Series
total_log <- ts(log(total_vehicles$TOTALNSA), start = c(1976,01), frequency = 12)
tseries::kpss.test(total_log, null = "Trend")
total_log %>% urca::ur.kpss() %>% summary()

auto_prod <- ts(log(auto_production$DAUPNSA), start = c(1993, 1), frequency = 12)
tseries::kpss.test(auto_prod, null = "Trend")
auto_prod %>% urca::ur.kpss() %>% summary()

rate <- ts(bank_rate$MPRIME, start = c(1949, 1), frequency = 12)
tseries::kpss.test(rate, null = "Trend")
rate %>% urca::ur.kpss() %>% summary()

unemp <- ts(unemployment$UNRATE, start = c(1948, 1), frequency = 12)
tseries::kpss.test(unemp, null = "Trend")
unemp %>% urca::ur.kpss() %>% summary()

gas_cpi <- ts(log(fuel_prices$CUSR0000SETB01), start = c(1968, 2), frequency = 12) 
tseries::kpss.test(gas_cpi, null = "Trend")
gas_cpi %>% urca::ur.kpss() %>% summary()


```

```{r, include=FALSE}
# ADF Tests for Time Series

tseries::adf.test(total_log)
tseries::adf.test(auto_prod)
tseries::adf.test(rate)
tseries::adf.test(unemp)
tseries::adf.test(gas_cpi)
```

```{r}
class <- c("Total Vehicles", "Domestic Production", "Bank Prime Rate", "Uemployment", "Gasoline CPI")
test_stat1 <- c(1.3497, 3.9468, 2.1478, 1.168, 8.2357)
significant_p <- c(0.01, 0.01, 0.01, 0.01, 0.01)

kpss.tib <- tibble(class, test_stat1, significant_p)

kpss.tib %>%
  gt::gt() %>%
  gt::tab_header(title = " KPSS Test Statistic and Significant Values of Time Series") %>%
  gt::cols_label(class = "Time Series",
             test_stat1 = "Test-Statistic",
             significant_p = "P-Values")
```

```{r}
class <- c("Total Vehicles", "Domestic Production", "Bank Prime Rate", "Uemployment", "Gasoline CPI")
test_stat1 <- c(-2.7392, -3.8473, -2.8386, -3.8808, -2.5347)
significant_p <- c(0.27, 0.02, 0.22, 0.01, 0.35)

adf.tib <- tibble(class, test_stat1, significant_p)

adf.tib %>%
  gt::gt() %>%
  gt::tab_header(title = "ADF Test Statistic and Significant Values of Time Series") %>%
  gt::cols_label(class = "Time Series",
             test_stat1 = "Test-Statistic",
             significant_p = "P-Values")
```

From the two tables, we see that we have slightly conflicting
information as to which time series are stationary. From the KPSS tests,
we see that totalall our variables have p-values less than 0.05, meaning
that we will have to difference the data to attempt to make our data
stationary. From our ADF test, we see that with the exception of our
domestic auto production and unemployment time series, all our other
time series are non-staionary. To be safe, data identified as
non-stationary by either tests will be differenced and then we will once
again conduct the KPSS and ADF tests on them to check for stationarity.

In the case of domestic auto production the following procedure will
first apply a differencing function and then plot the function to see if
the differencing allowed the data to become stationary.

```{r, fig.cap="Domestic Auto Production log transformed and differenced."}
plot(ts(difference(log(auto_production$DAUPNSA)), start = c(1993,01), frequency = 12), ylab = "Auto Production")
```

From the above plot, we see that with the exception of the spikes
brought about from COVID-19 production disruptions there does not appear
to be any defined pattern that this data follows, so we see that adding
one difference allows the time series to become stationary.

```{r, include=FALSE}
# KPSS Tests for Differenced Time Series
total_log <- ts(log(total_vehicles$TOTALNSA), start = c(1976,01), frequency = 12)
tseries::kpss.test(diff(total_log), null = "Trend")
diff(total_log) %>% urca::ur.kpss() %>% summary()

auto_prod <- ts(log(auto_production$DAUPNSA), start = c(1993, 1), frequency = 12)
tseries::kpss.test(diff(auto_prod), null = "Trend")
diff(auto_prod) %>% urca::ur.kpss() %>% summary()

rate <- ts(bank_rate$MPRIME, start = c(1949, 1), frequency = 12)
tseries::kpss.test(diff(rate), null = "Trend")
diff(rate) %>% urca::ur.kpss() %>% summary()

unemp <- ts(unemployment$UNRATE, start = c(1948, 1), frequency = 12)
tseries::kpss.test(diff(unemp), null = "Trend")
diff(unemp) %>% urca::ur.kpss() %>% summary()

gas_cpi <- ts(log(fuel_prices$CUSR0000SETB01), start = c(1968, 2), frequency = 12) 
tseries::kpss.test(diff(gas_cpi), null = "Trend")
diff(gas_cpi) %>% urca::ur.kpss() %>% summary()
```

```{r}
class <- c("Total Vehicles", "Domestic Production", "Bank Prime Rate", "Uemployment", "Gasoline CPI")
test_stat1 <- c(0.0297, 0.0181, 0.0715, 0.0527, 0.0614)
significant_p <- c(0.10, 0.10, 0.10, 0.10, 0.10)

kpss.tib <- tibble(class, test_stat1, significant_p)

kpss.tib %>%
  gt::gt() %>%
  gt::tab_header(title = " KPSS Test Statistic and Significant Values of Differenced Time Series") %>%
  gt::cols_label(class = "Time Series",
             test_stat1 = "Test-Statistic",
             significant_p = "P-Values")
```

```{r, include=FALSE}
# ADF Tests for Differenced Time Series

tseries::adf.test(diff(total_log))
tseries::adf.test(diff(auto_prod))
tseries::adf.test(diff(rate))
tseries::adf.test(diff(unemp))
tseries::adf.test(diff(gas_cpi))
```

```{r}
class <- c("Total Vehicles", "Domestic Production", "Bank Prime Rate", "Uemployment", "Gasoline CPI")
test_stat1 <- c(-13.599, -10.316, -9.1312, -9.683, -8.9804)
significant_p <- c(0.01, 0.01, 0.01, 0.01, 0.01)

adf.tib <- tibble(class, test_stat1, significant_p)

adf.tib %>%
  gt::gt() %>%
  gt::tab_header(title = "ADF Test Statistic and Significant Values of Differenced Time Series") %>%
  gt::cols_label(class = "Time Series",
             test_stat1 = "Test-Statistic",
             significant_p = "P-Values")
```

From the tables above, we see that all of our time series are now
appropriately differenced and have been made stationary. This will then
allow us to complete further analysis of our data.

We can now consider the type of model that we will use for our analysis,
the ARDL model. ARDL models are comprised of two main parts the AR, or
autoregressive portion of the model and the DL, or distributed lag
portion of the model.

We will first discuss autoregressive models. In autoregression, our goal
is to forecast the variable of interest using linear combinations of
previous values of the variable. This means that in an autoregression,
the variable is regressed on itself. An model that utilizes
autoregression of order p can be denoted by
$$y_t = c + \phi_1y_{t-1} + \phi_2y_{t-2} + \cdots + \phi_py_{t-p} + \epsilon_t$$,
where $\epsilon_t$ denotes white noise. In such a case, we would say
that our model is an AR(p) model. For these types of models, we face
certain constraints, mainly:

-   For an AR(1) model: $-1 < \phi_1 < 1$

-   For an AR(2) model:
    $-1 < \phi_2 < 1, \phi_1 + \phi_2 < 1, \phi_2-\phi_1 < 1$.

While autoregression models use past values of itself, distributed lag
models use the current and past values of a variable of interest using
linear combinations of the previous values of the variable. This means
that the variable of interest is regressed on current and previous
values of the independent variable. A distributed lag model of order r
DL(r) can be written as
$$Y_t = \alpha + \sum_{i=1}^rX_{t-i}+\epsilon)t$$

The full ARDL(P,r) model can be expressed as follows:
$$Y_t = \alpha +  \phi_1y_{t-1} + \cdots + \phi_py_{t-p} + \beta_0X_t + \cdots + \beta_rX_{t-r} + \epsilon_t$$
Now that we have our data properly differenced, we now need to come up
with our regression models. Our goal will be to regress each independent
varaiable individually on the total number of cars sold to try to
understand what relationship, if any is present between our economic
factors and the resulting number of vehicles sold.

The first thing we want to consider with our model is that the number of
total vehicles sold as shown in our data analysis demonstrated
seasonality so in order to account for that in our model, we will want
to use dummy coding. After taking seasonality into account, we can use
an ANVOA test to determine if the dummy variables results in a
significantly different model.

```{r}
total_log <- ts(log(total_vehicles$TOTALNSA), start = c(1976,01), frequency = 12)

diff_total_log <- diff(total_log)
model_base <- dynlm(diff_total_log ~ 1)
model_with_season <- dynlm(diff_total_log ~ season(diff_total_log), data = total_log)
ar_model <- dynlm(diff_total_log ~ L(diff_total_log, 1:12) + season(diff_total_log), data = diff_total_log)
anova(model_base, model_with_season) %>% gt::gt(rowname_col = "Model with Seasonality") %>% gt::tab_header(title = "Analyis of Variance Table") 
                                                                       
```

The small p-value indicates that the seasonal terms play a significant
role in the model so we should make sure to take that into account for
our models going forwards

### Total Vehicles Sold and Domestic Auto Production:

We can now consider the time series model when regressed against each
other. The first model we will look at is the ARDL model including total
vehicles sold and domestic auto production. When considering ARDL models
we must first attempt to calculate the number of lags which are
significant for each variable. To do this, I will elect to use Akaike's
Information Criterion (AIC) and Bayesian Information Criterion (BIC) and
the `auto_ardl` function which uses AIC and BIC for both variables
simultaneously to search for the best possible model. Information
criterion are of the form
$nlog(\frac{RSS}{n}) + kp, p = \# \text{ of parameters}$ With:

-   AIC, k = 2 $$nlog(\frac{RSS}{n}) + 2p$$

-   BIC, k = 2 $$nlog(\frac{RSS}{n}) + log(n)p$$

BIC tends to penalize larger models more heavily so if the lags required
for AIC models are quite high, we may choose to use the lags chosen
through BIC to prevent over fitting.

After Selecting the model that will best fit our data, we can then
perform residual analysis to ensure that our data results in the
remaining residuals having no autocorrelation present. This can be done
both with an inspection of residual plots and with the implmentation of
a Breusch-Godfrey test. Ideally, we want to see that there are no
patterns present in the residuals vs fitted plot, indicating that our
reisudals are evenly distributed that the scale-location plot has a flat
line, indicating constant variance, and that the points on the QQ-plot
lie close to the line. The Residuals vs. Leverage plot help us identify
influential points on the model.

The Breusch-Godfrey test is a statistical test that allow us to
determine if autocorrelations are present between the residuals of our
time series regression models. The test is conducted first by creating
the regression then considering the sample residuals as follows
$$u_t = \rho u_{t-1} + \dots + \rho_m u_{t-m} + \epsilon_t$$ and where
the null hypothesis is $$H_0 : \rho_1 = \rho_2 = \dots = \rho_m = 0.$$
If the sample size is large enough, then $nR^2 \sim \chi^2(p)$. If we do
not reject the null hypothesis, that means that there is no serial
correlation of order up to $p$ [@bgtest].

As a final measure of the effectiveness of our model, we will also apply
the Granger causality test to see if including lags of our regressor are
actually informative in terms of predicting Y [@econometricsbook]. The
null and alternative hypothesis of the Granger Causality Test is as
follows
$$H_0 = \text{ Time series X does not cause time series Y to Granger-cause itself.}$$
$$H_A = \text{ Time series X causes time series Y to Granger-cause itself.}$$
It is important to note that for a time series can Granger-cause another
variable if it is helpful for forecasting the other variable [@granger].

```{r, include = FALSE}
total_log <- ts(log(total_vehicles$TOTALNSA), start = c(1976,01), frequency = 12)
diff_total_log <- diff(total_log)

auto_prod <- ts(log(auto_production$DAUPNSA), start = c(1993, 1), frequency = 12)
auto_prod <- diff(auto_prod)

total_prod_analysis <- ts.intersect(diff_total_log, auto_prod)

auto_ardl(diff_total_log ~ auto_prod, data = total_prod_analysis, max_order = 24, selection = "BIC")
auto_ardl(diff_total_log ~ auto_prod, data = total_prod_analysis, max_order = 24)
auto_ardl(diff_total_log ~ auto_prod, data = total_prod_analysis, max_order = 24)
VARselect(diff_total_log, lag.max = 24)
VARselect(auto_prod, lag.max = 24)
```

```{r, fig.cap="Analysis of ARDL (12,10) on Total New Vehicles Sold with Domestic Auto Production as Exogenuous Variable."}
prod_total_modelvarbic <- dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:12) + L(auto_prod, 0:6) + season(diff_total_log), data = total_prod_analysis)

prod_total_model_auto <- dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:12) + L(auto_prod, 0:10) + season(diff_total_log), data = total_prod_analysis)


# checkresiduals(prod_total_modelvarbic) 
forecast::checkresiduals(prod_total_model_auto)

# par(mfrow = c(2,2))
# plot(prod_total_modelvarbic)


par(mfrow = c(2,2))
plot(prod_total_model_auto)
```

From the outputs, above showing our residual analysis, we can see that
the residual values we obtain are consistent with being white noise and
that all our values appear normally distributed. Additionally, the
p-value we got from the Breusch-Godfrey test was 0.0879 \> 0.05,
therefore we can conclude that it is likely that our residuals are not
serially correlated. We can then conclude that our current model is
appropriate with modeling the true data. We can now test to see if
adding the lags for domestic auto production were helpful in predicting
the number of vehicles sold.

```{r, include = FALSE}
prod_total_model_auto <- dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:12) + L(auto_prod, 0:10) + season(diff_total_log), data = total_prod_analysis)
prod_total_noprod <- dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:12) + season(diff_total_log), data = total_prod_analysis)

anova(prod_total_model_auto, prod_total_noprod) 
```

From the Granger-Causality test conducted above, we get a p-value \<
0.05, therefore we can reject the null hypothesis and conclude that it
is likely that domestic auto production Granger-causes the number of
vehicles sold. For the following time series, the same process will be
followed with the output being listed in a table.

```{r}
prod_act_fit <- plot(stats::diffinv(total_prod_analysis[,"diff_total_log"], lag = 1, difference = 1, xi = 6.819252)) +
  lines(stats::diffinv(-fitted(prod_total_model_auto), lag = 1, difference = 1, xi = 6.952251), col = 2) + title("Actual vs Fitted for Domestic Auto Production and Total Vehicles Sold")
```

### Total Vehicles Sold and Unemployment Rate:

```{r}
unemp <- ts(unemployment$UNRATE, start = c(1948, 1), frequency = 12)
unemp <- diff(unemp)
total_unemp_analysis <- ts.intersect(diff_total_log, unemp)
```

```{r, include = FALSE}
ARDL::auto_ardl(diff_total_log ~ unemp, data = total_unemp_analysis, max_order = 24, selection = "BIC")
ARDL::auto_ardl(diff_total_log ~ unemp, data = total_unemp_analysis, max_order = 24)
vars::VARselect(unemp, lag.max = 24)
vars::VARselect(diff_total_log, lag.max = 24)

unemp_total_modelvarbic <-  dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:12) + L(unemp, 0:1) + season(diff_total_log), data = total_unemp_analysis)
unemp_total_model_autobic <- dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:12) + L(unemp, 0:12) + season(diff_total_log), data = total_unemp_analysis)
unemp_total_model_autoaic <- dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:15) + L(unemp, 0:14) + season(diff_total_log), data = total_unemp_analysis)

checkresiduals(unemp_total_model_autobic)
checkresiduals(unemp_total_model_autoaic)
checkresiduals(unemp_total_modelvarbic)
```

```{r, include = FALSE}
unemp_total_modelvar_bic <-  dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:12) + L(unemp, 0:1) + season(diff_total_log), data = total_unemp_analysis)

checkresiduals(unemp_total_modelvar_bic)
```

```{r, include = FALSE}
unemp_total_nounemp <- dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:12) + season(total_log), data = total_unemp_analysis)

anova(unemp_total_modelvar_bic, unemp_total_nounemp) 
```

```{r}
model_emp <- c("ARDL(12,1)", "ARDL(12, 14)", "ARDL(15,14)")
selection_crit_emp <- c("Individually Selected BIC", "Auto_ARDL BIC", "Auto_ARDL AIC")
significant_p_emp <- c(0.2617, 0.04512, 0.00733)
emp_adequacy <- c("Adequate", "Inadequate", "Inadequate")
emp_granger_causality_F <- c(16.105, NULL, NULL)
emp_granger_p <- c("< 0.01", "", "")

emp.tib <- tibble(model_emp, selection_crit_emp, significant_p_emp, emp_adequacy, emp_granger_p)

emp.tib %>%
  gt::gt() %>%
  gt::tab_header(title = "Total Vehicle Sales and Unemployment Model Analysis (Breusch-Godfrey Test)") %>%
  gt::cols_label(model_emp = "Model",
             selection_crit_emp = "Selection Criteria",
             significant_p_emp = "P-Values",
             emp_adequacy = "Model Adequacy",
             emp_granger_p = "Granger Causality P-Value")
```

From the 3 models that we tried, the best one was the model that was the
ARDL(12,1) model, meaning we used 12 lags of total auto sales and 1 lag
of the unemployment rate. For the Granger Causality test we also
obtained a p-value that was smaller than 0.05 which means we can
conclude that it is likely that Unemployment Granger-causes total auto
sales.

```{r}
unemp_total_modelvar_bic <-  dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:12) + L(unemp, 0:1) + season(diff_total_log), data = total_unemp_analysis)
unemp_total_nounemp <- dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:12) + season(total_log), data = total_unemp_analysis)

anova(unemp_total_modelvar_bic, unemp_total_nounemp) 
```

```{r}
unemp_act_fit <- plot(diffinv(total_unemp_analysis[,"diff_total_log"], lag = 1, difference = 1, xi = 6.785814)) +
  lines(diffinv(-fitted(unemp_total_modelvar_bic), difference = 1, xi = 6.877193), col = 2) + title("Actual vs Fitted for Unemployment and Total Vehicles Sold")
```

### Total Vehicles Sold and Gas CPI:

```{r}
gas_cpi <- ts(log(fuel_prices$CUSR0000SETB01), start = c(1968, 2), frequency = 12) 

gas_cpi <- diff(gas_cpi)

total_gas_analysis <- ts.intersect(diff_total_log, gas_cpi)
```

```{r, include = FALSE}
ARDL::auto_ardl(diff_total_log ~ gas_cpi, data = total_gas_analysis, max_order = 24, selection = "BIC")
ARDL::auto_ardl(diff_total_log ~ gas_cpi, data = total_gas_analysis, max_order = 24)
vars::VARselect(gas_cpi, lag.max = 24)
vars::VARselect(diff_total_log, lag.max = 24)

gas_total_modelvarbic <-  dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:12) + L(gas_cpi, 0:2) + season(diff_total_log), data = total_gas_analysis)
gas_total_model_autobic <- dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:12) + L(gas_cpi, 0:12) + season(diff_total_log), data = total_gas_analysis)
gas_total_model_autoaic <- dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:15) + L(gas_cpi, 0:12) + season(diff_total_log), data = total_gas_analysis)

checkresiduals(gas_total_modelvarbic)
checkresiduals(gas_total_model_autobic)
checkresiduals(gas_total_model_autoaic)
```

```{r, include = FALSE}
gas_total_nogas <- dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:12) + season(total_log), data = total_gas_analysis)

anova(gas_total_model_autobic, gas_total_nogas) 
```

```{r}
model_gas <- c("ARDL(12,2)", "ARDL(12, 12)", "ARDL(15,12)")
selection_crit_gas <- c("Individually Selected BIC", "Auto_ARDL BIC", "Auto_ARDL AIC")
significant_p_gas <- c(0.0331, 0.1262, 0.03477)
gas_adequacy <- c("Inadequate", "Adequate", "Inadequate")
gas_granger_p <- c("", "< 0.01", "")

gas.tib <- tibble(model_gas, selection_crit_gas, significant_p_gas, gas_adequacy, gas_granger_p)

gas.tib %>%
  gt::gt() %>%
  gt::tab_header(title = "Total Vehicle Sales and Fuel CPI Model Analysis (Breusch-Godfrey Test)") %>%
  gt::cols_label(model_gas = "Model",
             selection_crit_gas = "Selection Criteria",
             significant_p_gas = "P-Values",
             gas_adequacy = "Model Adequacy",
             gas_granger_p = "Granger Causality P-Value")
```

From the 3 models that we tried, the best one was the model that was
selected by `auto_ardl` using BIC as the selection criterion. The best
model is ARDL(12,12) model, meaning we used 12 lags of total auto sales
and 12 lag of the gasoline CPI. For the Granger Causality test we also
obtained a p-value that was smaller than 0.05 which means we can
conclude that it is likely that gasoline prices Granger-causes total
auto sales.

```{r}
gas_act_fit <- plot(diffinv(total_gas_analysis[,"diff_total_log"], lag = 1, difference = 1, xi = 6.785814)) +
  lines(diffinv(-fitted(gas_total_model_autobic), lag = 1, difference = 1, xi = 6.877193), col = 2) + title("Actual vs Fitted for Gas and Total Vehicles Sold")
```

### Total Vehicles Sold and Bank Prime Rate Loan:

```{r}
rate <- ts(bank_rate$MPRIME, start = c(1949, 1), frequency = 12)

rate <- diff(rate)

total_rate_analysis <- ts.intersect(diff_total_log, rate)
```

```{r, include = FALSE}
ARDL::auto_ardl(diff_total_log ~ rate, data = total_rate_analysis, max_order = 24, selection = "BIC")
ARDL::auto_ardl(diff_total_log ~ rate, data = total_rate_analysis, max_order = 24)
vars::VARselect(rate, lag.max = 24)
vars::VARselect(diff_total_log, lag.max = 24)

rate_total_modelvarbic <-  dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:12) + L(rate, 0:13) + season(total_log), data = total_rate_analysis)
rate_total_model_autobic <- dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:12) + rate + season(total_log), data = total_rate_analysis)
rate_total_model_autoaic <- dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:14) + L(rate, 0:13) + season(total_log), data = total_rate_analysis)

checkresiduals(rate_total_model_autobic)
checkresiduals(rate_total_model_autoaic)
checkresiduals(rate_total_modelvarbic)
```

```{r, include = FALSE}
rate_total_norate <- dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:12) + season(total_log), data = total_rate_analysis)

anova(rate_total_model_autobic, rate_total_norate) 
```

```{r}
model_rate <- c("ARDL(12,13)", "ARDL(12, 0)", "ARDL(14,13)")
selection_crit_rate <- c("Individually Selected BIC", "Auto_ARDL BIC", "Auto_ARDL AIC")
significant_p_rate <- c(0.01152, 0.05509, 0.0008)
rate_adequacy <- c("Indequate", "Adequate", "Inadequate")
rate_granger_p <- c("", "0.4755", "")

rate.tib <- tibble(model_rate, selection_crit_rate, significant_p_rate, rate_adequacy, rate_granger_p)

rate.tib %>%
  gt::gt() %>%
  gt::tab_header(title = "Total Vehicle Sales and Bank Prime Rates Model Analysis (Breusch-Godfrey Test)") %>%
  gt::cols_label(model_rate = "Model",
             selection_crit_rate = "Selection Criteria",
             significant_p_rate = "P-Values",
             rate_adequacy = "Model Adequacy",
             rate_granger_p = "Granger Causality P-Value")
```

From the 3 models that we tried, the best one was the model that was
selected by `auto_ardl` using BIC as the selection criterion. The best
model is ARDL(12,0) model, meaning we used 12 lags of total auto sales
and the current value of the gasoline CPI. For the Granger Causality
test we obtained a p-value that $0.4755 > 0.05$ so we cannot reject the
null hypothesis, which means that it is unlikely that bank prime rate
loans Granger-cause total auto sales.

```{r}
rate_act_fit <- plot(diffinv(total_rate_analysis[,"diff_total_log"], lag = 1, difference = 1, xi = 6.785814)) +
  lines(diffinv(-fitted(rate_total_model_autobic), lag = 1, difference = 1, xi = 6.877193), col = 2) + title("Actual vs Fitted for Bank Prime Rate and Total Vehicles Sold")
```

```{r}
par(mfrow = c(2,2)) 
plot(diffinv(total_rate_analysis[,"diff_total_log"], lag = 1, difference = 1, xi = 6.785814)) +
  lines(diffinv(-fitted(rate_total_model_autobic), lag = 1, difference = 1, xi = 6.877193), col = 2) + title("Actual vs Fitted for Bank Prime Rate and Total Vehicles Sold")
plot(diffinv(total_unemp_analysis[,"diff_total_log"], lag = 1, difference = 1, xi = 6.785814)) +
  lines(diffinv(-fitted(unemp_total_modelvar_bic), difference = 1, xi = 6.877193), col = 2) + title("Actual vs Fitted for Unemployment and Total Vehicles Sold")
plot(diffinv(total_gas_analysis[,"diff_total_log"], lag = 1, difference = 1, xi = 6.785814)) +
  lines(diffinv(-fitted(gas_total_model_autobic), lag = 1, difference = 1, xi = 6.877193), col = 2) + title("Actual vs Fitted for Gas and Total Vehicles Sold")
plot(stats::diffinv(total_prod_analysis[,"diff_total_log"], lag = 1, difference = 1, xi = 6.819252)) +
  lines(stats::diffinv(-fitted(prod_total_model_auto), lag = 1, difference = 1, xi = 6.952251), col = 2) + title("Actual vs Fitted for Domestic Auto Production and Total Vehicles Sold")
```

## Forecast Analysis

Now that we have appropriate models, we can analyze the forecasts
provided by the models. In order to do this, we will utilize thge method
of pseudo out-of-sample forecasting which follows the following steps
[@econometricsbook]: 1.) Divide the sample data into $s = T - P$ and $P$
subsequent observations where the $P$ observations are used as
pseudo-out-of-sample observations

2.) Estimate the model using the first $s$ observations.

3.) Compute the pseudo-forecast $\tilde{Y}_{s+1|s}$.

4.) Compute the pseudo-forecast-error $\tilde{u}_{s+1} = Y_{s+1|s}$.

Doing this will then allow us to determine how our models forecast and
which models appear to have the best forecasting performance when fit to
actual data. Since it appears that the bank prime rate loan does not
appear to have a significant effect on the total number of cars sold, we
will analyze the forecasts of ARDL models including the unemployment
rate, gasoline CPI, and domestic auto production.

### Unemployment Forecast

```{r}
EndOfSample <- seq(2020, 2023, 1/12)

# initialize matrix forecasts
forecasts <- matrix(nrow = 1, 
                    ncol = length(EndOfSample))

# initialize vector SER
SER  <- numeric(length(EndOfSample))

# estimation loop over end of sample dates
for(i in 1:length(EndOfSample)) {

  # estimate Uenemployment Model
  
  unemp_total_modelvar_bic <-  dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:12) + L(unemp, 0:1), start = c(1976,1) + season(diff_total_log), end = EndOfSample[i], data = total_unemp_analysis)
  
  
  SER[i] <- summary(unemp_total_modelvar_bic)$sigma

  # sample data for one-period ahead forecast
  s <- window(total_unemp_analysis, EndOfSample[i] - 13/12, EndOfSample[i])

  # compute forecast
  forecasts[i] <- coef(unemp_total_modelvar_bic) %*% c(1, s[2, 1], s[3, 1], s[4,1], s[5,1], s[6,1], s[7,1], s[8,1], s[9,1], s[10,1], s[11,1], s[12,1], s[13, 1], s[12, 2], s[13,2])
}
# compute psuedo-out-of-sample forecast errors
POOSFCE <- c(window(diff_total_log, c(2020, 1), c(2023, 1))) - forecasts

# series of pseudo-out-of-sample forecasts
PSOSSFc <- ts(c(forecasts), 
              start = 2020, 
              end = 2023, 
              frequency = 12)

# plot the GDP growth time series
plot(window(diff_total_log, c(2020, 1), c(2023, 1)),
     col = "steelblue",
     lwd = 2,
     ylab = "Percent",
     main = "Pseudo-Out-Of-Sample Forecasts of Difference in Auto Production") +

# add the series of pseudo-out-of-sample forecasts
lines(PSOSSFc, 
      lwd = 2, 
      lty = 2)  +

# shade area between curves (the pseudo forecast error)
polygon(c(time(PSOSSFc), rev(time(PSOSSFc))), 
        c(window(diff_total_log, c(2020, 1), c(2023, 1)), rev(PSOSSFc)),
        col = alpha("blue", alpha = 0.3),
        border = NA) +

# add a legend
legend("bottomleft", 
       lty = c(1, 2, 1),
       lwd = c(2, 2, 10),
       col = c("steelblue", "black", alpha("blue", alpha = 0.3)), 
       legend = c("Actual GDP growth rate",
         "Forecasted GDP growth rate",
         "Pseudo forecast Error"))

SER[1]
sd(POOSFCE)
```

### Gas Forecast

```{r}
EndOfSample_gas <- seq(2020, 2023, 1/12)

# initialize matrix forecasts
forecasts_gas <- matrix(nrow = 1, 
                    ncol = length(EndOfSample_gas))

# initialize vector SER
SER_gas  <- numeric(length(EndOfSample_gas))

# estimation loop over end of sample dates
for(i in 1:length(EndOfSample_gas)) {

# estimate Gas Model
  gas_total_model_autobic <- dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:12) + L(gas_cpi, 0:12) + season(diff_total_log), start = c(1976,1), end = EndOfSample_gas[i], data = total_gas_analysis)
  
  
  SER_gas[i] <- summary(gas_total_model_autobic)$sigma

  # sample data for one-period ahead forecast
  s <- window(total_gas_analysis, EndOfSample_gas[i] - 13/12, EndOfSample_gas[i])

  # compute forecast
  forecasts_gas[i] <- coef( gas_total_model_autobic) %*% c(1, s[1, 1], s[2, 1], s[3,1], s[4,1], s[5,1], s[6,1], s[7,1], s[8,1], s[9,1], s[10,1], s[11,1], s[12, 1], s[12, 2], s[11,2])
}
sd# compute psuedo-out-of-sample forecast errors
POOSFCE_gas <- c(window(diff_total_log, c(2020, 1), c(2023, 1))) - forecasts_gas

# series of pseudo-out-of-sample forecasts
PSOSSFc_gas <- ts(c(forecasts_gas), 
              start = 2020, 
              end = 2023, 
              frequency = 12)

# plot the GDP growth time series
plot(window(diff_total_log, c(2020, 1), c(2023, 1)),
     col = "steelblue",
     lwd = 2,
     ylab = "Percent",
     main = "Pseudo-Out-Of-Sample Forecasts of GDP Growth") +

# add the series of pseudo-out-of-sample forecasts
lines(PSOSSFc_gas, 
      lwd = 2, 
      lty = 2)  +

# shade area between curves (the pseudo forecast error)
polygon(c(time(PSOSSFc_gas), rev(time(PSOSSFc_gas))), 
        c(window(diff_total_log, c(2020, 1), c(2023, 1)), rev(PSOSSFc_gas)),
        col = alpha("blue", alpha = 0.3),
        border = NA) 

# add a legend
legend("bottomleft", 
       lty = c(1, 2, 1),
       lwd = c(2, 2, 10),
       col = c("steelblue", "black", alpha("blue", alpha = 0.3)), 
       legend = c("Actual GDP growth rate",
         "Forecasted GDP growth rate",
         "Pseudo forecast Error"))

SER[1]
sd(POOSFCE)
```

### Domestic Auto Production Forecast

```{r}
EndOfSample_prod <- seq(2020, 2023, 1/12)

# initialize matrix forecasts
forecasts_prod <- matrix(nrow = 1, 
                    ncol = length(EndOfSample_prod))

# initialize vector SER
SER_prod  <- numeric(length(EndOfSample_prod))

# estimation loop over end of sample dates
for(i in 1:length(EndOfSample_prod)) {

# estimate Gas Model
  prod_total_model_auto <- dynlm::dynlm(diff_total_log ~ L(diff_total_log, 1:12) + L(auto_prod, 0:10) + season(diff_total_log), start = c(1993,1), end = EndOfSample_prod[i], data = total_prod_analysis)
  
  SER_prod[i] <- summary(prod_total_model_auto)$sigma

  # sample data for one-period ahead forecast
  s_prod <- window(total_prod_analysis, EndOfSample_prod[i] - 13/12, EndOfSample_prod[i])

  # compute forecast
  forecasts_prod[i] <- coef(prod_total_model_auto) %*% c(1, s[1, 1], s[2, 1], s[3,1], s[4,1], s[5,1], s[6,1], s[7,1], s[8,1], s[9,1], s[10,1], s[11,1], s[12, 1], s[12, 2], s[11,2])
}
sd# compute psuedo-out-of-sample forecast errors
POOSFCE_prod <- c(window(diff_total_log, c(2020, 1), c(2023, 1))) - forecasts_prod

# series of pseudo-out-of-sample forecasts
PSOSSFc_prod <- ts(c(forecasts_prod), 
              start = 2020, 
              end = 2023, 
              frequency = 12)

# plot the GDP growth time series
plot(window(diff_total_log, c(2020, 1), c(2023, 1)),
     col = "steelblue",
     lwd = 2,
     ylab = "Percent",
     main = "Pseudo-Out-Of-Sample Forecasts of GDP Growth") +

# add the series of pseudo-out-of-sample forecasts
lines(PSOSSFc_prod, 
      lwd = 2, 
      lty = 2)  +

# shade area between curves (the pseudo forecast error)
polygon(c(time(PSOSSFc_prod), rev(time(PSOSSFc_prod))), 
        c(window(diff_total_log, c(2020, 1), c(2023, 1)), rev(PSOSSFc_prod)),
        col = alpha("blue", alpha = 0.3),
        border = NA) 

# add a legend
legend("bottomleft", 
       lty = c(1, 2, 1),
       lwd = c(2, 2, 10),
       col = c("steelblue", "black", alpha("blue", alpha = 0.3)), 
       legend = c("Actual GDP growth rate",
         "Forecasted GDP growth rate",
         "Pseudo forecast Error"))
```
## Discussion

## Conclusion
This project focused on 


## References
